<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jon Barron</title>

    <meta name="author" content="Jonathan Bodea">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jon Barron
                </p>
                <p>
                  I'm a high school student at <a href="https://johnscreek.fultonschools.org/">Johns Creek High School</a> in Georgia.
                  I am passionate about AI, especially AI safety, where I am working on designing algorithms to evaluate and mitigate deceptive behavior in AI models, with the goal of ensuring they align with human values.
                  I have also worked my way up to the Gold Division in <a href="https://usaco.org/">USACO</a>, where I have gained extensive algorithmic knowledge and problem-solving skills. Currently, I am spending every waking moment on advancing towards USACO Platinum and learning everything I can about AI (it's kind of fun, though).
                </p>
                <p style="text-align:center">
                  <a href="mailto:jonathan.bodea@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://x.com/jonathanbodea?s=21">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jbodea">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JonBarron.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
<!--         <h2>News</h2>

            <ul>
                <li><b>July 2025</b>: I presented LMRL-Gym at ICML 2025.
                <li><b>January 2025</b>: I became a Cooperative AI PHD Fellow.
                <li><b>July 2024</b>: Moral Foundations for LLMs was presented at EMNLP 2024.
                <li><b>September 2022</b>: I became head GSI for CS285: Deep Reinforcement Learning at UC Berkeley.
                <li><b>September 2021</b>: I began my PhD at UC Berkeley!
                <li><b>January 2021</b>: I led a program sponsored by Helping Hand Relief & Development to teach refugee students in Jordan how to code in Java. Check out course content <a href="https://hhrd-cs.github.io/">here</a>.
                <li><b>December 2020</b>: I gave a TEDxMIT Talk on the importance of interfaith dialouge and its connection to my research. Check it out <a href="https://www.youtube.com/watch?v=31L9eYsfp6E&ab_channel=TEDxTalks">here</a>.
                <li><b>September 2020</b>: I began my Masters at MIT in EECS!

            </ul> -->
        <h2>Preprints</h2>

<!--<h3>Reinforcement Learning</h3>-->          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/consistency.png" alt="lmrl_gym" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://www.linkedin.com/in/ryanyicheng" target="_blank">Ryan Cheng</a>,
              <a href="https://www.linkedin.com/in/donovanclay" target="_blank">Donovan Clay</a>,
              <a href="https://homes.cs.washington.edu/~althoff/" target="_blank">Tim Althoff</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a>,
              <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
              <br>
              <em>NeurIPS 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2511.00222" target="_blank">Paper</a> /
              <a href="https://github.com/abdulhaim/consistent-LLMs" target="_blank">Code</a> /
              <a href="https://sites.google.com/view/consistent-llms" target="_blank">Website</a>
              <p>Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving consistency in LLM-generated dialogue, reducing inconsistency by over 55\%, resulting in more coherent, faithful, and trustworthy simulated users.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/motivation_rl_v3.png" alt="deception" style="border-style: none" width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Deception in Dialogue: Evaluating and Mitigating Deceptive Behavior in Large Language Models.</papertitle>
                  <br>
                  <strong>Marwa Abdulhai</strong>,
                  <a href="https://www.linkedin.com/in/ryanyicheng" target="_blank">Ryan Cheng</a>,
                  <a href="https://www.linkedin.com/in/aryansh-s" target="_blank">Aryansh Shrivastava</a>,
                  <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>,
                  <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/", target="_blank">Yarin Gal</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">Sergey Levine</a>.
                  <br>
                  <em>Pre-print.</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2510.14318" target="_blank">Paper</a> /
                  <a href="https://github.com/abdulhaim/deceptive_dialogue" target="_blank">Code</a> /
                  <a href="https://sites.google.com/view/deceptive-dialogue" target="_blank">Website</a>
                  <p>Large Language Models (LLMs) interact with hundreds of millions of people worldwide, powering applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. 
                  In this paper, we systematically investigate the extent to which LLMs engage in deception within dialogue. We benchmark 8 state-of-the-art models on 4 dialogue tasks, showing that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.

                </p>
                </td>
              </tr>
            </tbody></table>

        <h2>Selected Publications</h2>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lmrl_gym.png" alt="lmrl_gym" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models.</papertitle>
              <br>
                <strong>Marwa Abdulhai</strong>,
                <a href="https://icwhite.github.io/website/" target="_blank">Isadora White</a>,
                <a href="https://sea-snell.github.io/" target="_blank">C. Snell</a>,
                <a href="https://www.linkedin.com/in/charlesjsun/", target="_blank">C. Sun</a>,
                <a href="https://jxihong.github.io/joeyhong/", target="_blank">J. Hong</a>,
                <a href="https://yx-s-z.github.io/", target="_blank">Y. Zhai</a>,
                <a href="https://kelvinxu.github.io/", target="_blank">K. Xu</a>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">S. Levine</a>.
              <br>
              <em>ICLR GenAI4DM Workshop 2024; ICML 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2311.18232" target="_blank">Paper</a> /
              <a href="https://github.com/abdulhaim/LMRL-Gym" target="_blank">Code</a> /
              <a href="https://lmrl-gym.github.io/" target="_blank">Website</a>
              <p>We introduce LMRL‑Gym, a benchmark and toolkit for developing reinforcement learning algorithms that operate with large language models (LLMs) across eight multi-turn dialogue and text-game tasks. This work demonstrates training LLMs to plan and act strategically, ask clarifying questions, and solve long-horizon tasks—bridging reactive generation and goal-driven interaction.</p>
            </td>
          </tr>
        </tbody></table>


    <!--<h3>LLMs for Social Science</h3>-->          


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/defining_deception.png" alt="deception" style="border-style: none" width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Defining Deception in Decision Making.</papertitle>
                  <br>
                  <strong>Marwa Abdulhai</strong>,
                  <a href="https://micahcarroll.github.io/" target="_blank">Micah Carroll</a>,
                  <a href="https://justinsvegliato.com/" target="_blank">Justin Svegliato</a>,
                  <a href="https://people.eecs.berkeley.edu/~anca/", target="_blank">Anca Dragan</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">Sergey Levine</a>.
                  <br>
                  <em>AAMAS 2024</em>
                  <br>
                  <a href="https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2111.pdf" target="_blank">Paper</a>
                  <p>This paper formalizes deception in decision-making processes and proposes principled methods to detect and evaluate deceptive trajectories in multi-agent systems, using game-theoretic and reinforcement learning approaches.</p>
                </td>
              </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/davinci_model.png" alt="moral foundations" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Moral Foundations of Large Language Models.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://scholar.google.com/citations?user=fa4EXucAAAAJ&hl=en" target="_blank">Gregory Serapio-García</a>,
              <a href="https://scholar.google.com/citations?user=UrmqI6IAAAAJ&hl=en" target="_blank">Clement Crepy</a>,
              <a href="https://www.linkedin.com/in/ddwalter/" target="_blank">Daria Valter</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/canny.html", target="_blank">John Canny</a>,
              <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
              <br>
              <em>AAAI-23 R2HCAI (Best Paper); EMNLP 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2310.15337" target="_blank">Paper</a> /
              <a href="https://github.com/abdulhaim/moral_foundations_llms" target="_blank">Code</a> /
              <a href="https://sites.google.com/view/moral-foundations-llms" target="_blank">Website</a>
              <p>Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model's behavior on downstream tasks.</p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/virtual_personas.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Virtual Personas for Language Models via an Anthology of Backstories.</papertitle>
                <br>
                <a href="https://suhongmoon.github.io/" target="_blank">S. Moon*</a>,
                <strong>Marwa Abdulhai*</strong>,
                <a href="https://joshuaminwookang.github.io/" target="_blank">M. Kang*</a>,
                <a href="https://josephsuh.org/" target="_blank">J. Suh*</a>,
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/canny.html", target="_blank">John Canny</a>.
                <br>
                <em>EMNLP 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2407.06576" target="_blank">Paper</a> /
                <a href="https://github.com/CannyLab/anthology" target="_blank">Code</a> 
                <p>Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. In this work, we introduce "Anthology", a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as "backstories." We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics.</p>
              </td>
            </tr>
          </tbody></table>

    <!--<h3>AI Safety</h3>--> 

        <h2>Other Publications</h2>
         
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/concordia.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Evaluating Generalization Capabilities of LLM Agents in Mixed-Motive Scenarios Using Concordia</papertitle>
                <br>
                <a href="https://chandlersmith.me/" target="_blank">Chandler Smith</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://manfreddiaz.github.io/" target="_blank">Manfred Diaz</a>,
                [80 authors],
                <a href="https://people.csail.mit.edu/dhm/", target="_blank">D. Hadfield-Menell</a>,
                <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>,
                <a href="https://scholar.google.com/citations?user=n9AWbcAAAAAJ&hl=en", target="_blank">J. Hernandez-Orallo</a>.
                <a href="https://www.jzleibo.com/", target="_blank">Joel Leibo</a>.
                <br>
                <em>Pre-print</em>
                <br>
                <a href="https://www.cooperativeai.com/contests/concordia-2024" target="_blank">Contest</a> /
                <a href="https://github.com/google-deepmind/concordia" target="_blank">Code</a> 
                <p>Large language model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. This work introduces an approach to measuring human-appropriate cooperative intelligence, emphasizing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/curiosity.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</papertitle>
                <br>
                <a href="https://simon-wan.github.io/" target="_blank">Yanming Wan</a>,
                <a href="https://www.linkedin.com/in/jiaxing-wu-187624143" target="_blank">Jiaxing Wu</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://scholar.google.co.il/citations?user=TrQLB1gAAAAJ&hl=iw" target="_blank">Lior Shani</a>,
                <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
                <br>
                <em>2nd Workshop on Models of Human Feedback for AI Alignment at ICML 2025.</em>
                <br>
                <a href="https://arxiv.org/abs/2504.03206" target="_blank">Paper</a> 
                <p>Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. We show improved generalization capabilities compared to standard multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.</p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/islam_ai_faact.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Blind Faith? User Preference and Expert Assessment of AI-Generated Religious Content.</papertitle>
                <br>
                <a href="https://sites.google.com/view/sabriyaalam" target="_blank">Sabriya Alam</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://niloufar.org/" target="_blank">Niloufar Salehi</a>.
                <br>
                <em>FAccT 2025</em>
                <br>
                <a href="https://dl.acm.org/doi/full/10.1145/3715275.3732162" target="_blank">Paper</a> 
                <p>The increasing use of AI tools like ChatGPT for religious information-seeking among Muslims raises critical questions about the intersection of technology, faith, and expertise. This mixed-methods study investigates user and expert evaluations of AI-generated religious content, through both quantitative and qualitative analysis of user preferences and expert feedback elicited through interviews, surveys, and expert consultations. Our findings reveal a significant disconnect: despite expressing distrust in AI for religious guidance and stating a preference for scholarly answers, Muslim users overwhelmingly preferred AI-generated responses to Islamic questions in blind evaluations, favoring them in 81.3% of cases.</p>
              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cradol.png" alt="cradol" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Context-Specific Representation Abstraction for Deep Option Learning.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://dkkim93.github.io/", target="_blank">Dongki Kim</a>,
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>,
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>,
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              <em>AAAI-22</em>
              <br>
              <a href="https://arxiv.org/abs/2109.09876", target="_blank">Paper</a> /
              <a href="https://github.com/cradol/cradol", target="_blank">Code</a> /
              <a href="https://sites.google.com/view/cradol/home", target="_blank">Video</a>
              <p></p>
              <p></p>
              <p>
              We introduce Context-Specific Representation Abstraction for Deep Option Learning (CRADOL), a new framework that considers both temporal abstraction and context-specific representation abstraction to effectively reduce the size of the search over policy space.
              </p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/meta_mapg.png" alt="meta_mapg" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning.</papertitle>
              <br>
              <a href="https://dkkim93.github.io/", target="_blank">Dongki Kim</a>,
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>,
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>,
              <a href="https://scholar.google.com/citations?user=BCbAD0UAAAAJ&hl=en" target="_blank">Chuangchuang Sun</a>,
              <strong>Marwa Abdulhai</strong>,
              <a href="https://scholar.google.com/citations?user=hU-LeNEAAAAJ&hl=en" target="_blank">Golnaz Habibi</a>,
              <a href="http://acl.mit.edu/people/slcot" target="_blank">Sebastian Lopez-Cot</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>,
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              <em>ICML-21, AAAI-20 Symposium</em>
              <br>
              <a href="https://arxiv.org/pdf/2011.00382.pdf", target="_blank">Paper</a> /
              <a href="https://github.com/dkkim93/meta-mapg", target="_blank">Code</a> /
              <a href="https://sites.google.com/view/meta-mapg/home", target="_blank">Video</a>
              <p></p>
              <p></p>
              <p>
                We develop a novel meta-multiagent policy gradient theorem that directly accommodates for the non-stationary policy dynamics inherent to multiagent settings. Our meta-agent directly considers both an agent’s own non-stationary policy dynamics and the non-stationary policy dynamics of other agents to adapt fast.
              </p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <h2>Teaching</h2>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr onmouseout="font_stop()" onmouseover="font_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/cs188.png" alt="planning" style="border-style: none" width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>Graduate Student Instructor for CS188: Artificial Intelligence </papertitle>
            <br>
            Spring 2019
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp24/", target="_blank">Course Website</a> /
            <p></p>
            <p></p>
          </td>
        </tr>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="font_stop()" onmouseover="font_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/deepRL.png" alt="planning" style="border-style: none" width="200">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
           <papertitle>Head Graduate Student Instructor
           CS285 Deep Reinforcement Learning</papertitle>
          <br>
          Fall 2022
          <br>
          <a href="https://rail.eecs.berkeley.edu/deeprlcourse/", target="_blank">Course Website</a> /
          <p></p>
          <p></p>
        </td>
      </tr>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/planning.png" alt="planning" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Teaching Assistant for 6.141: Robotics Science & Systems</papertitle>
              <br>
              Spring 2019
              <br>
              <a href="https://github.com/mit-rss", target="_blank">Assignments</a> /
              <a href="https://medium.com/@marwaabdulhai/6-141-review-7e1d548055ab", target="_blank">Medium Article</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/react.jpg" alt="react" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Student Instructor for MIT Refugee Action Hub</papertitle>
              <br>
              January 2019
              <br>
              <a href="https://react.mit.edu/", target="_blank">Program Page</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/009graph.png" alt="009graph" style="border-style: none" width="200" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Lab Assistant for 6.009: Fundamentals of Programming in Python</papertitle>
              <br>
              December 2018
              <br>
              <a href="https://py.mit.edu/fall20", target="_blank">Course Page</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>